{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c07af53-32e2-448f-a618-18c79a11a187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+-----+\n|order_id|   category|quantity|price|\n+--------+-----------+--------+-----+\n|     101|Electronics|       1|15000|\n|     102|Electronics|       2|12000|\n|     103|    Fashion|       3|  800|\n|     104|    Fashion|       1| 1500|\n|     105|    Grocery|       5|  200|\n|     106|    Grocery|       2|  300|\n|     107|Electronics|       1|50000|\n|     108|    Fashion|       4|  600|\n+--------+-----------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (101, \"Electronics\", 1, 15000),\n",
    "    (102, \"Electronics\", 2, 12000),\n",
    "    (103, \"Fashion\", 3, 800),\n",
    "    (104, \"Fashion\", 1, 1500),\n",
    "    (105, \"Grocery\", 5, 200),\n",
    "    (106, \"Grocery\", 2, 300),\n",
    "    (107, \"Electronics\", 1, 50000),\n",
    "    (108, \"Fashion\", 4, 600)\n",
    "]\n",
    "\n",
    "cols = [\"order_id\", \"category\", \"quantity\", \"price\"]\n",
    "\n",
    "df = spark.createDataFrame(data, cols)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd0f5947-3a81-4610-9275-d8a39d603c2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **_Aggregation Functions_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8168a95-d583-4b1b-9275-d034c950fdcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------+---------+------------+\n|total_price|max_price|min_price|avg_price|total_record|\n+-----------+---------+---------+---------+------------+\n|      80400|    50000|      200|  10050.0|           8|\n+-----------+---------+---------+---------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Importing all required PySpark SQL functions\n",
    "# sum, max, min, avg, count, etc.\n",
    "# -------------------------------------------------------------------\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Performing aggregation on the entire DataFrame\n",
    "# agg() without groupBy() → applies aggregations on ALL rows\n",
    "# -------------------------------------------------------------------\n",
    "df.agg(\n",
    "\n",
    "    # Sum of all price values\n",
    "    sum(\"price\").alias(\"total_price\"),\n",
    "\n",
    "    # Maximum price value\n",
    "    max(\"price\").alias(\"max_price\"),\n",
    "\n",
    "    # Minimum price value\n",
    "    min(\"price\").alias(\"min_price\"),\n",
    "\n",
    "    # Average of all price values\n",
    "    avg(\"price\").alias(\"avg_price\"),\n",
    "\n",
    "    # Count of non-null price values\n",
    "    count(\"price\").alias(\"total_record\")\n",
    "\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dd98f79-1e27-4866-ae65-fe542cb4f171",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **_Group by_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4499bc6-7582-46ae-a130-d0888d07792b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---------+---------+---------+------------+\n|   category|total_price|max_price|min_price|avg_price|total_record|\n+-----------+-----------+---------+---------+---------+------------+\n|Electronics|      77000|    50000|    12000| 25666.67|           3|\n|    Fashion|       2900|     1500|      600|   966.67|           3|\n|    Grocery|        500|      300|      200|    250.0|           2|\n+-----------+-----------+---------+---------+---------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Grouping data by \"category\" and applying aggregate functions\n",
    "# groupBy() → groups rows based on the given column(s)\n",
    "# agg()     → applies multiple aggregations on grouped data\n",
    "# -------------------------------------------------------------------\n",
    "df.groupBy(\"category\").agg(\n",
    "\n",
    "    # Total sum of price for each category\n",
    "    sum(\"price\").alias(\"total_price\"),\n",
    "\n",
    "    # Maximum price in each category\n",
    "    max(\"price\").alias(\"max_price\"),\n",
    "\n",
    "    # Minimum price in each category\n",
    "    min(\"price\").alias(\"min_price\"),\n",
    "\n",
    "    # Average price rounded to 2 decimal places\n",
    "    round(avg(\"price\"), 2).alias(\"avg_price\"),\n",
    "\n",
    "    # Count how many price records exist in each category\n",
    "    count(\"price\").alias(\"total_record\")\n",
    "\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc9dbc64-9dda-49db-916e-89c596219147",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---------+---------+---------+------------+\n|   category|total_price|max_price|min_price|avg_price|total_record|\n+-----------+-----------+---------+---------+---------+------------+\n|Electronics|      77000|    50000|    12000| 25666.67|           3|\n+-----------+-----------+---------+---------+---------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Performing aggregation grouped by \"category\"\n",
    "# and filtering results based on aggregated values\n",
    "# -------------------------------------------------------------------\n",
    "df.groupBy(\"category\").agg(\n",
    "\n",
    "    # Total price for each category\n",
    "    sum(\"price\").alias(\"total_price\"),\n",
    "\n",
    "    # Highest price in each category\n",
    "    max(\"price\").alias(\"max_price\"),\n",
    "\n",
    "    # Lowest price in each category\n",
    "    min(\"price\").alias(\"min_price\"),\n",
    "\n",
    "    # Average price rounded to 2 decimal places\n",
    "    round(avg(\"price\"), 2).alias(\"avg_price\"),\n",
    "\n",
    "    # Total number of price records\n",
    "    count(\"price\").alias(\"total_record\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Filtering after aggregation\n",
    "# Keep only those categories where total_price > 5000\n",
    "# (Filtering must come AFTER agg(), not before)\n",
    "# -------------------------------------------------------------------\n",
    ").filter(\n",
    "    col(\"total_price\") > 5000\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1b614f4-74f5-4e5b-95f6-9bd9ae54c61d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **_Windows Function_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e984274-0c04-482d-b557-b0defff5c730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+-----+\n|student_id| name|subject|marks|\n+----------+-----+-------+-----+\n|         1|Rahul|   Math|   78|\n|         1|Rahul|Science|   75|\n|         1|Rahul|English|   69|\n|         2|Priya|   Math|   92|\n|         2|Priya|Science|   81|\n|         2|Priya|English|   95|\n|         3| Amit|   Math|   65|\n|         3| Amit|Science|   72|\n|         3| Amit|English|   69|\n|         4|Sneha|   Math|   90|\n|         4|Sneha|Science|   85|\n|         4|Sneha|English|   78|\n|         5|Rohan|   Math|   90|\n|         5|Rohan|Science|   75|\n|         5|Rohan|English|   68|\n+----------+-----+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (1, \"Rahul\", \"Math\", 78),\n",
    "    (1, \"Rahul\", \"Science\", 75),\n",
    "    (1, \"Rahul\", \"English\", 69),\n",
    "    (2, \"Priya\", \"Math\", 92),\n",
    "    (2, \"Priya\", \"Science\", 81),\n",
    "    (2, \"Priya\", \"English\", 95),\n",
    "    (3, \"Amit\", \"Math\", 65),\n",
    "    (3, \"Amit\", \"Science\", 72),\n",
    "    (3, \"Amit\", \"English\", 69),\n",
    "    (4, \"Sneha\", \"Math\", 90),\n",
    "    (4, \"Sneha\", \"Science\", 85),\n",
    "    (4, \"Sneha\", \"English\", 78),\n",
    "    (5, \"Rohan\", \"Math\", 90),\n",
    "    (5, \"Rohan\", \"Science\", 75),\n",
    "    (5, \"Rohan\", \"English\", 68),\n",
    "]\n",
    "\n",
    "columns = [\"student_id\", \"name\", \"subject\", \"marks\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65b74572-d2a8-4c1a-8821-abd9ca77d9b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+-----+----------+----+------+\n|student_id| name|subject|marks|row_number|rank|d_rank|\n+----------+-----+-------+-----+----------+----+------+\n|         2|Priya|English|   95|         1|   1|     1|\n|         4|Sneha|English|   78|         2|   2|     2|\n|         1|Rahul|English|   69|         3|   3|     3|\n|         3| Amit|English|   69|         4|   3|     3|\n|         5|Rohan|English|   68|         5|   5|     4|\n|         2|Priya|   Math|   92|         1|   1|     1|\n|         4|Sneha|   Math|   90|         2|   2|     2|\n|         5|Rohan|   Math|   90|         3|   2|     2|\n|         1|Rahul|   Math|   78|         4|   4|     3|\n|         3| Amit|   Math|   65|         5|   5|     4|\n|         4|Sneha|Science|   85|         1|   1|     1|\n|         2|Priya|Science|   81|         2|   2|     2|\n|         1|Rahul|Science|   75|         3|   3|     3|\n|         5|Rohan|Science|   75|         4|   3|     3|\n|         3| Amit|Science|   72|         5|   5|     4|\n+----------+-----+-------+-----+----------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Importing required functions and Window specification module\n",
    "# -------------------------------------------------------------------\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Creating a window specification\n",
    "# partitionBy(\"subject\") → group rows by subject\n",
    "# orderBy(marks desc)   → sort within each subject group by marks (highest first)\n",
    "# This window will be applied to ranking functions\n",
    "# -------------------------------------------------------------------\n",
    "windowSpec = Window.partitionBy(\"subject\").orderBy(col(\"marks\").desc())\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Applying window functions to create new ranking columns\n",
    "# 1) row_number()  → gives unique row number per partition (no ties)\n",
    "# 2) rank()        → same rank for ties but gaps appear\n",
    "# 3) dense_rank()  → same rank for ties but no gaps\n",
    "# -------------------------------------------------------------------\n",
    "df.withColumn(\"row_number\", row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"rank\", rank().over(windowSpec)) \\\n",
    "  .withColumn(\"dense_rank\", dense_rank().over(windowSpec)) \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc763793-a3b7-49e8-a997-77b0f29737bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+-----+---------+------+---------+-----------+\n|student_id| name|subject|marks|prev_mark|diff_m|next_mark|Total_marks|\n+----------+-----+-------+-----+---------+------+---------+-----------+\n|         3| Amit|Science|   72|     NULL|  NULL|       69|         72|\n|         3| Amit|English|   69|       72|    -3|       65|        141|\n|         3| Amit|   Math|   65|       69|    -4|     NULL|        206|\n|         2|Priya|English|   95|     NULL|  NULL|       92|         95|\n|         2|Priya|   Math|   92|       95|    -3|       81|        187|\n|         2|Priya|Science|   81|       92|   -11|     NULL|        268|\n|         1|Rahul|   Math|   78|     NULL|  NULL|       75|         78|\n|         1|Rahul|Science|   75|       78|    -3|       69|        153|\n|         1|Rahul|English|   69|       75|    -6|     NULL|        222|\n|         5|Rohan|   Math|   90|     NULL|  NULL|       75|         90|\n|         5|Rohan|Science|   75|       90|   -15|       68|        165|\n|         5|Rohan|English|   68|       75|    -7|     NULL|        233|\n|         4|Sneha|   Math|   90|     NULL|  NULL|       85|         90|\n|         4|Sneha|Science|   85|       90|    -5|       78|        175|\n|         4|Sneha|English|   78|       85|    -7|     NULL|        253|\n+----------+-----+-------+-----+---------+------+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Creating a window specification\n",
    "# Partition the data by 'name' and order rows by marks (descending)\n",
    "# This window will be used for LEAD, LAG, and window aggregation\n",
    "# ---------------------------------------------------------------\n",
    "windowspace = Window.partitionBy(\"name\").orderBy(col(\"marks\").desc())\n",
    "\n",
    "df.withColumn(\n",
    "\n",
    "    # LAG → Get previous row's marks within the same window (previous student attempt)\n",
    "    \"prev_mark\", lag(\"marks\").over(windowspace)\n",
    "\n",
    ").withColumn(\n",
    "\n",
    "    # Difference between current marks and previous marks\n",
    "    # Useful for finding improvement or drop in marks\n",
    "    \"diff_m\", col(\"marks\") - col(\"prev_mark\")\n",
    "\n",
    ").withColumn(\n",
    "\n",
    "    # LEAD → Get next row's marks within the same window (next student attempt)\n",
    "    \"next_mark\", lead(\"marks\").over(windowspace)\n",
    "\n",
    ").withColumn(\n",
    "\n",
    "    # Window aggregation: Total marks for each 'name'\n",
    "    # SUM OVER WINDOW → Running total or category-wise total without collapsing rows\n",
    "    \"Total_marks\", sum(\"marks\").over(windowspace)\n",
    "\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79933d0d-c644-476e-9bd0-ca72d1b9a367",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **_Joins_**\n",
    "- Inner Join : Only Matching Data From Both Datafrmae\n",
    "- Outer join \n",
    "    - Left : Matching From Both Tables + Addional Rwos From Left table\n",
    "    - Right : Matching From Both Tables + Addional Rwos From Right table\n",
    "    - Full\n",
    "- Self Join\n",
    "- Cross Join\n",
    "- Semi Join\n",
    "- Anti Join      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0013615c-1771-46d4-814d-a316df5f53e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+\n|student_id| name|gender|\n+----------+-----+------+\n|         1|Rahul|     M|\n|         2|Priya|     F|\n|         3| Amit|     M|\n|         4|Sneha|     F|\n|         6|Kiran|     M|\n+----------+-----+------+\n\n+---------+-----------+\n|course_id|course_name|\n+---------+-----------+\n|      101|       Math|\n|      102|    Science|\n|      103|    English|\n|      104|   Computer|\n|      105|    Biology|\n+---------+-----------+\n\n+----------+---------+-----+\n|student_id|course_id|marks|\n+----------+---------+-----+\n|         1|      101|   78|\n|         1|      102|   88|\n|         2|      101|   92|\n|         2|      103|   95|\n|         3|      101|   65|\n|         4|      103|   78|\n|         5|      101|   70|\n+----------+---------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "students_data = [\n",
    "    (1, \"Rahul\", \"M\"),\n",
    "    (2, \"Priya\", \"F\"),\n",
    "    (3, \"Amit\", \"M\"),\n",
    "    (4, \"Sneha\", \"F\"),\n",
    "    (6, \"Kiran\", \"M\"),  # no enrollment\n",
    "]\n",
    "\n",
    "courses_data = [\n",
    "    (101, \"Math\"),\n",
    "    (102, \"Science\"),\n",
    "    (103, \"English\"),\n",
    "    (104, \"Computer\"),\n",
    "    (105, \"Biology\"),  # no enrollment\n",
    "]\n",
    "\n",
    "enrollments_data = [\n",
    "    (1, 101, 78),\n",
    "    (1, 102, 88),\n",
    "    (2, 101, 92),\n",
    "    (2, 103, 95),\n",
    "    (3, 101, 65),\n",
    "    (4, 103, 78),\n",
    "    (5, 101, 70),  # student not in students table\n",
    "]\n",
    "\n",
    "students_cols = [\"student_id\", \"name\", \"gender\"]\n",
    "courses_cols = [\"course_id\", \"course_name\"]\n",
    "enrollments_cols = [\"student_id\", \"course_id\", \"marks\"]\n",
    "\n",
    "students_df = spark.createDataFrame(students_data, students_cols)\n",
    "courses_df = spark.createDataFrame(courses_data, courses_cols)\n",
    "enrollments_df = spark.createDataFrame(enrollments_data, enrollments_cols)\n",
    "\n",
    "students_df.show()\n",
    "courses_df.show()\n",
    "enrollments_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64877726-001e-4699-9e51-122721a69c3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+-----+------+\n|student_id|course_id|marks| name|gender|\n+----------+---------+-----+-----+------+\n|         1|      102|   88|Rahul|     M|\n|         1|      101|   78|Rahul|     M|\n|         2|      103|   95|Priya|     F|\n|         2|      101|   92|Priya|     F|\n|         3|      101|   65| Amit|     M|\n|         4|      103|   78|Sneha|     F|\n+----------+---------+-----+-----+------+\n\n+----------+---------+-----+-----+------+\n|student_id|course_id|marks| name|gender|\n+----------+---------+-----+-----+------+\n|         1|      101|   78|Rahul|     M|\n|         1|      102|   88|Rahul|     M|\n|         2|      101|   92|Priya|     F|\n|         2|      103|   95|Priya|     F|\n|         3|      101|   65| Amit|     M|\n|         4|      103|   78|Sneha|     F|\n|         5|      101|   70| NULL|  NULL|\n+----------+---------+-----+-----+------+\n\n+----------+---------+-----+-----+------+\n|student_id|course_id|marks| name|gender|\n+----------+---------+-----+-----+------+\n|         1|      102|   88|Rahul|     M|\n|         1|      101|   78|Rahul|     M|\n|         2|      103|   95|Priya|     F|\n|         2|      101|   92|Priya|     F|\n|         3|      101|   65| Amit|     M|\n|         4|      103|   78|Sneha|     F|\n|         6|     NULL| NULL|Kiran|     M|\n+----------+---------+-----+-----+------+\n\n+----------+---------+-----+-----+------+\n|student_id|course_id|marks| name|gender|\n+----------+---------+-----+-----+------+\n|         1|      101|   78|Rahul|     M|\n|         1|      102|   88|Rahul|     M|\n|         2|      101|   92|Priya|     F|\n|         2|      103|   95|Priya|     F|\n|         3|      101|   65| Amit|     M|\n|         4|      103|   78|Sneha|     F|\n|         5|      101|   70| NULL|  NULL|\n|         6|     NULL| NULL|Kiran|     M|\n+----------+---------+-----+-----+------+\n\n+----------+---------+-----+\n|student_id|course_id|marks|\n+----------+---------+-----+\n|         1|      101|   78|\n|         1|      102|   88|\n|         2|      101|   92|\n|         2|      103|   95|\n|         3|      101|   65|\n|         4|      103|   78|\n+----------+---------+-----+\n\n+----------+---------+-----+\n|student_id|course_id|marks|\n+----------+---------+-----+\n|         5|      101|   70|\n+----------+---------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "enrollments_df.join(students_df,\"student_id\",\"inner\").show()\n",
    "\n",
    "enrollments_df.join(students_df,\"student_id\",\"left\").show()\n",
    "\n",
    "enrollments_df.join(students_df,\"student_id\",\"right\").show()\n",
    "\n",
    "enrollments_df.join(students_df,\"student_id\",\"full\").show() # all the rows from both the tables\n",
    "\n",
    "enrollments_df.join(students_df,\"student_id\",\"left_semi\").show() # left semi join --> matching from left table only - like the wich are in left table only\n",
    "\n",
    "enrollments_df.join(students_df,\"student_id\",\"left_anti\").show() # left anit join --> only non matching record from left table - the row which are not match from left table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ceca3d7b-0ec9-4c41-9886-6cebffebd580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Class5 - Transformation 2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}