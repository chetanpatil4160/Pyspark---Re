{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dc5c5ef-bf24-4974-8870-f6033999b5c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **_Activity 1 - Selecting Specific Columns_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bee91bd-0494-4649-b1b9-d05ab2e68f55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---------+----------+----------+\n| id|  name|salary|  address|department|joineddate|\n+---+------+------+---------+----------+----------+\n|  1|  John| 50000|   Mumbai|        IT|2021-01-15|\n|  2|  Emma| 62000|    Delhi|        HR|2020-03-10|\n|  3|   Raj| 45000|     Pune|   Finance|2022-07-20|\n|  4| Priya| 70000|Bangalore|        IT|2019-11-05|\n|  5|  Alex| 55000|Hyderabad| Marketing|2021-05-12|\n|  6|  Sara| 48000|  Chennai|   Finance|2020-08-25|\n|  7|Nikhil| 72000|Ahmedabad|        HR|2022-02-14|\n|  8| Aisha| 53000|  Kolkata|        IT|2021-09-30|\n|  9|   Tom| 60000|    Surat| Marketing|2023-01-18|\n| 10|  Riya| 58000|   Jaipur|   Finance|2020-12-02|\n| 11|Vikram| 65000|   Nagpur|        HR|2019-04-22|\n| 12|  Zara| 49000|   Indore|        IT|2022-10-11|\n+---+------+------+---------+----------+----------+\n\n+------+---------+\n|  name|  address|\n+------+---------+\n|  John|   Mumbai|\n|  Emma|    Delhi|\n|   Raj|     Pune|\n| Priya|Bangalore|\n|  Alex|Hyderabad|\n|  Sara|  Chennai|\n|Nikhil|Ahmedabad|\n| Aisha|  Kolkata|\n|   Tom|    Surat|\n|  Riya|   Jaipur|\n|Vikram|   Nagpur|\n|  Zara|   Indore|\n+------+---------+\n\n+------+---------+-------+\n|  name|  address|  bonus|\n+------+---------+-------+\n|  John|   Mumbai|25000.0|\n|  Emma|    Delhi|31000.0|\n|   Raj|     Pune|22500.0|\n| Priya|Bangalore|35000.0|\n|  Alex|Hyderabad|27500.0|\n|  Sara|  Chennai|24000.0|\n|Nikhil|Ahmedabad|36000.0|\n| Aisha|  Kolkata|26500.0|\n|   Tom|    Surat|30000.0|\n|  Riya|   Jaipur|29000.0|\n|Vikram|   Nagpur|32500.0|\n|  Zara|   Indore|24500.0|\n+------+---------+-------+\n\n+----------+\n|first_name|\n+----------+\n|      John|\n|      Emma|\n|       Raj|\n|     Priya|\n|      Alex|\n|      Sara|\n|    Nikhil|\n|     Aisha|\n|       Tom|\n|      Riya|\n|    Vikram|\n|      Zara|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Loading Data into DataFrame\n",
    "# -----------------------------\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .load(\"/Volumes/workspace/default/my_data/employee_data.csv\")  # File path\n",
    "\n",
    "# Displaying the loaded data\n",
    "df.show()\n",
    "\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Selecting specific columns using select()\n",
    "# -----------------------------------------------\n",
    "df1 = df.select(\"name\", \"address\")   # Selecting columns directly by name\n",
    "df1.show()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Selecting columns using selectExpr()\n",
    "# selectExpr allows SQL expressions inside select()\n",
    "# Useful when we want to apply calculations or transformations\n",
    "# ---------------------------------------------------------\n",
    "df2 = df.selectExpr(\n",
    "    \"name\",\n",
    "    \"address\",\n",
    "    \"salary * 0.5 as bonus\"   # Creating a new column using an expression\n",
    ")\n",
    "\n",
    "df2.show()\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Selecting a column using col() function\n",
    "# col() is useful when we want to apply functions,\n",
    "# transformations, or rename columns using alias()\n",
    "# -------------------------------------------------\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df3 = df.select(\n",
    "    col(\"name\").alias(\"first_name\")   # Renaming 'name' column to 'first_name'\n",
    ")\n",
    "\n",
    "# Displaying the result\n",
    "df3.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a9c32f9-5a76-40c9-abb9-2e16fe60bf6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **_Activity 2 - Adding New Columns_**\n",
    "### - **_Using withColumn_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9a2ed1d-4902-47b5-9080-165d51b91621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---------+----------+----------+------+\n| id|  name|salary|  address|department|joineddate| bonus|\n+---+------+------+---------+----------+----------+------+\n|  1|  John| 50000|   Mumbai|        IT|2021-01-15|2500.0|\n|  2|  Emma| 62000|    Delhi|        HR|2020-03-10|3100.0|\n|  3|   Raj| 45000|     Pune|   Finance|2022-07-20|2250.0|\n|  4| Priya| 70000|Bangalore|        IT|2019-11-05|3500.0|\n|  5|  Alex| 55000|Hyderabad| Marketing|2021-05-12|2750.0|\n|  6|  Sara| 48000|  Chennai|   Finance|2020-08-25|2400.0|\n|  7|Nikhil| 72000|Ahmedabad|        HR|2022-02-14|3600.0|\n|  8| Aisha| 53000|  Kolkata|        IT|2021-09-30|2650.0|\n|  9|   Tom| 60000|    Surat| Marketing|2023-01-18|3000.0|\n| 10|  Riya| 58000|   Jaipur|   Finance|2020-12-02|2900.0|\n| 11|Vikram| 65000|   Nagpur|        HR|2019-04-22|3250.0|\n| 12|  Zara| 49000|   Indore|        IT|2022-10-11|2450.0|\n+---+------+------+---------+----------+----------+------+\n\n+---+------+------+---------+----------+----------+-------+\n| id|  name|salary|  address|department|joineddate|country|\n+---+------+------+---------+----------+----------+-------+\n|  1|  John| 50000|   Mumbai|        IT|2021-01-15|  India|\n|  2|  Emma| 62000|    Delhi|        HR|2020-03-10|  India|\n|  3|   Raj| 45000|     Pune|   Finance|2022-07-20|  India|\n|  4| Priya| 70000|Bangalore|        IT|2019-11-05|  India|\n|  5|  Alex| 55000|Hyderabad| Marketing|2021-05-12|  India|\n|  6|  Sara| 48000|  Chennai|   Finance|2020-08-25|  India|\n|  7|Nikhil| 72000|Ahmedabad|        HR|2022-02-14|  India|\n|  8| Aisha| 53000|  Kolkata|        IT|2021-09-30|  India|\n|  9|   Tom| 60000|    Surat| Marketing|2023-01-18|  India|\n| 10|  Riya| 58000|   Jaipur|   Finance|2020-12-02|  India|\n| 11|Vikram| 65000|   Nagpur|        HR|2019-04-22|  India|\n| 12|  Zara| 49000|   Indore|        IT|2022-10-11|  India|\n+---+------+------+---------+----------+----------+-------+\n\n+---+------+------+---------+----------+----------+\n| id|  name|salary|  address|department|joineddate|\n+---+------+------+---------+----------+----------+\n|  1|  John| 50000|   MUMBAI|        IT|2021-01-15|\n|  2|  Emma| 62000|    DELHI|        HR|2020-03-10|\n|  3|   Raj| 45000|     PUNE|   Finance|2022-07-20|\n|  4| Priya| 70000|BANGALORE|        IT|2019-11-05|\n|  5|  Alex| 55000|HYDERABAD| Marketing|2021-05-12|\n|  6|  Sara| 48000|  CHENNAI|   Finance|2020-08-25|\n|  7|Nikhil| 72000|AHMEDABAD|        HR|2022-02-14|\n|  8| Aisha| 53000|  KOLKATA|        IT|2021-09-30|\n|  9|   Tom| 60000|    SURAT| Marketing|2023-01-18|\n| 10|  Riya| 58000|   JAIPUR|   Finance|2020-12-02|\n| 11|Vikram| 65000|   NAGPUR|        HR|2019-04-22|\n| 12|  Zara| 49000|   INDORE|        IT|2022-10-11|\n+---+------+------+---------+----------+----------+\n\n+---+------+------+---------+----------+----------+\n| id|  name|salary|  address|department|joineddate|\n+---+------+------+---------+----------+----------+\n|  1|  John| 50000|   Mumbai|        IT|2021-01-15|\n|  2|  Emma| 62000|    Delhi|        HR|2020-03-10|\n|  3|   Raj| 45000|     Pune|   Finance|2022-07-20|\n|  4| Priya| 70000|Bangalore|        IT|2019-11-05|\n|  5|  Alex| 55000|Hyderabad| Marketing|2021-05-12|\n|  6|  Sara| 48000|  Chennai|   Finance|2020-08-25|\n|  7|Nikhil| 72000|Ahmedabad|        HR|2022-02-14|\n|  8| Aisha| 53000|  Kolkata|        IT|2021-09-30|\n|  9|   Tom| 60000|    Surat| Marketing|2023-01-18|\n| 10|  Riya| 58000|   Jaipur|   Finance|2020-12-02|\n| 11|Vikram| 65000|   Nagpur|        HR|2019-04-22|\n| 12|  Zara| 49000|   Indore|        IT|2022-10-11|\n+---+------+------+---------+----------+----------+\n\n+---+------+------+---------+----------+----------+------+-------+\n| id|  name|salary|  address|department|joineddate| bonus|country|\n+---+------+------+---------+----------+----------+------+-------+\n|  1|  John| 50000|   Mumbai|        IT|2021-01-15|2500.0|  India|\n|  2|  Emma| 62000|    Delhi|        HR|2020-03-10|3100.0|  India|\n|  3|   Raj| 45000|     Pune|   Finance|2022-07-20|2250.0|  India|\n|  4| Priya| 70000|Bangalore|        IT|2019-11-05|3500.0|  India|\n|  5|  Alex| 55000|Hyderabad| Marketing|2021-05-12|2750.0|  India|\n|  6|  Sara| 48000|  Chennai|   Finance|2020-08-25|2400.0|  India|\n|  7|Nikhil| 72000|Ahmedabad|        HR|2022-02-14|3600.0|  India|\n|  8| Aisha| 53000|  Kolkata|        IT|2021-09-30|2650.0|  India|\n|  9|   Tom| 60000|    Surat| Marketing|2023-01-18|3000.0|  India|\n| 10|  Riya| 58000|   Jaipur|   Finance|2020-12-02|2900.0|  India|\n| 11|Vikram| 65000|   Nagpur|        HR|2019-04-22|3250.0|  India|\n| 12|  Zara| 49000|   Indore|        IT|2022-10-11|2450.0|  India|\n+---+------+------+---------+----------+----------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Adding a new column \"bonus\"\n",
    "# withColumn() is used to create or replace a column\n",
    "# Here, bonus = 5% of salary\n",
    "# -------------------------------------------------------------------\n",
    "df.withColumn(\"bonus\", col(\"salary\") * 0.05).show()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Adding a constant value column using lit()\n",
    "# lit(\"India\") creates a column with the same value for all rows\n",
    "# -------------------------------------------------------------------\n",
    "df.withColumn(\"country\", lit(\"India\")).show()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Modifying an existing column\n",
    "# If the column already exists, withColumn() will overwrite it\n",
    "# Here, converting address to uppercase using upper()\n",
    "# -------------------------------------------------------------------\n",
    "df.withColumn(\"address\", upper(col(\"address\"))).show()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Changing data type of a column using cast()\n",
    "# Converting \"id\" column to integer type\n",
    "# -------------------------------------------------------------------\n",
    "df.withColumn(\"id\", col(\"id\").cast(\"int\")).show()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Adding multiple columns at the same time by chaining withColumn()\n",
    "# 1) Creating bonus column\n",
    "# 2) Adding country column with constant value\n",
    "# -------------------------------------------------------------------\n",
    "(df.withColumn(\"bonus\", col(\"salary\") * 0.05) \n",
    "  .withColumn(\"country\", lit(\"India\")) \n",
    "  .show())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "338e1097-8bc2-447d-b375-8bc8813675a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **_Activity 3 - Droping A Columns_**\n",
    "### - **Using Drop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0035e90-3f0e-4143-a296-50e2a1281596",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----------+\n| id|  name|salary|joineddate|\n+---+------+------+----------+\n|  1|  John| 50000|2021-01-15|\n|  2|  Emma| 62000|2020-03-10|\n|  3|   Raj| 45000|2022-07-20|\n|  4| Priya| 70000|2019-11-05|\n|  5|  Alex| 55000|2021-05-12|\n|  6|  Sara| 48000|2020-08-25|\n|  7|Nikhil| 72000|2022-02-14|\n|  8| Aisha| 53000|2021-09-30|\n|  9|   Tom| 60000|2023-01-18|\n| 10|  Riya| 58000|2020-12-02|\n| 11|Vikram| 65000|2019-04-22|\n| 12|  Zara| 49000|2022-10-11|\n+---+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Dropping columns from a DataFrame\n",
    "# drop() can remove one or multiple columns at the same time\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Dropping multiple columns: \"department\" and \"address\"\n",
    "df2 = df.drop(\"department\", \"address\")\n",
    "df2.show()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Example: Dropping a single column\n",
    "# df.drop(\"department\").show()\n",
    "# -------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2d1e53d-e93a-4fbc-aed8-9416e9e263ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **_Activity 4 - Renameing A Columns_**\n",
    "### - **Using withColumnRenamed**\n",
    "### - **Using toDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf41b73a-b026-455e-b114-5d701d6f2f40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---------+----------+----------+\n|emp_id|emp_name|salary|  address|department|joineddate|\n+------+--------+------+---------+----------+----------+\n|     1|    John| 50000|   Mumbai|        IT|2021-01-15|\n|     2|    Emma| 62000|    Delhi|        HR|2020-03-10|\n|     3|     Raj| 45000|     Pune|   Finance|2022-07-20|\n|     4|   Priya| 70000|Bangalore|        IT|2019-11-05|\n|     5|    Alex| 55000|Hyderabad| Marketing|2021-05-12|\n|     6|    Sara| 48000|  Chennai|   Finance|2020-08-25|\n|     7|  Nikhil| 72000|Ahmedabad|        HR|2022-02-14|\n|     8|   Aisha| 53000|  Kolkata|        IT|2021-09-30|\n|     9|     Tom| 60000|    Surat| Marketing|2023-01-18|\n|    10|    Riya| 58000|   Jaipur|   Finance|2020-12-02|\n|    11|  Vikram| 65000|   Nagpur|        HR|2019-04-22|\n|    12|    Zara| 49000|   Indore|        IT|2022-10-11|\n+------+--------+------+---------+----------+----------+\n\n+------+--------+-------+---------+---------+------------+\n|emp_id|emp_name|emp_sal|  emp_add| emp_dept|emp_joindate|\n+------+--------+-------+---------+---------+------------+\n|     1|    John|  50000|   Mumbai|       IT|  2021-01-15|\n|     2|    Emma|  62000|    Delhi|       HR|  2020-03-10|\n|     3|     Raj|  45000|     Pune|  Finance|  2022-07-20|\n|     4|   Priya|  70000|Bangalore|       IT|  2019-11-05|\n|     5|    Alex|  55000|Hyderabad|Marketing|  2021-05-12|\n|     6|    Sara|  48000|  Chennai|  Finance|  2020-08-25|\n|     7|  Nikhil|  72000|Ahmedabad|       HR|  2022-02-14|\n|     8|   Aisha|  53000|  Kolkata|       IT|  2021-09-30|\n|     9|     Tom|  60000|    Surat|Marketing|  2023-01-18|\n|    10|    Riya|  58000|   Jaipur|  Finance|  2020-12-02|\n|    11|  Vikram|  65000|   Nagpur|       HR|  2019-04-22|\n|    12|    Zara|  49000|   Indore|       IT|  2022-10-11|\n+------+--------+-------+---------+---------+------------+\n\n+------+--------+----------+-----------+--------------+--------------+\n|emp_id|emp_name|emp_salary|emp_address|emp_department|emp_joineddate|\n+------+--------+----------+-----------+--------------+--------------+\n|     1|    John|     50000|     Mumbai|            IT|    2021-01-15|\n|     2|    Emma|     62000|      Delhi|            HR|    2020-03-10|\n|     3|     Raj|     45000|       Pune|       Finance|    2022-07-20|\n|     4|   Priya|     70000|  Bangalore|            IT|    2019-11-05|\n|     5|    Alex|     55000|  Hyderabad|     Marketing|    2021-05-12|\n|     6|    Sara|     48000|    Chennai|       Finance|    2020-08-25|\n|     7|  Nikhil|     72000|  Ahmedabad|            HR|    2022-02-14|\n|     8|   Aisha|     53000|    Kolkata|            IT|    2021-09-30|\n|     9|     Tom|     60000|      Surat|     Marketing|    2023-01-18|\n|    10|    Riya|     58000|     Jaipur|       Finance|    2020-12-02|\n|    11|  Vikram|     65000|     Nagpur|            HR|    2019-04-22|\n|    12|    Zara|     49000|     Indore|            IT|    2022-10-11|\n+------+--------+----------+-----------+--------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Renaming specific columns using withColumnRenamed()\n",
    "# We can rename one or multiple columns by chaining the function\n",
    "# -------------------------------------------------------------------\n",
    "df.withColumnRenamed(\"id\", \"emp_id\") \\\n",
    "  .withColumnRenamed(\"name\", \"emp_name\") \\\n",
    "  .show()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Renaming ALL columns at once using toDF()\n",
    "# toDF() requires the same number of new column names as the existing ones\n",
    "# -------------------------------------------------------------------\n",
    "df.toDF(\"emp_id\", \"emp_name\", \"emp_sal\", \"emp_add\", \"emp_dept\", \"emp_joindate\").show()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Getting all column names from the DataFrame\n",
    "# df.columns returns a Python list of column names\n",
    "# -------------------------------------------------------------------\n",
    "df.columns\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Renaming all columns programmatically\n",
    "# Adding prefix \"emp_\" to every column name\n",
    "# Using list comprehension to modify column names dynamically\n",
    "# -------------------------------------------------------------------\n",
    "data = [\"emp_\" + i for i in df.columns]  # Creating new column name list\n",
    "df.toDF(*data).show()                    # Unpacking list using *\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Without using * (WRONG)\n",
    "# df.toDF(data) look like this df.toDF([\"emp_id\", \"emp_name\", \"emp_sal\"])\n",
    "# Here we pass a single list as one argument -> toDF() will fail\n",
    "# ------------------------------------------------------------\n",
    "# Using *data (CORRECT)\n",
    "# The * operator UNPACKS the list into separate arguments:\n",
    "# [\"emp_id\", \"emp_name\", \"emp_sal\"]\n",
    "# becomes\n",
    "# \"emp_id\", \"emp_name\", \"emp_sal\"\n",
    "# df.toDF(*data) becomes df.toDF(\"emp_id\", \"emp_name\", \"emp_sal\")\n",
    "# ------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa5f254c-711f-4d3d-bc26-e10e1ba3bf71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **_Activity 5 - Removing Duplicates_**\n",
    "### - **Using distinct**\n",
    "### - **Using dropDuplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "790fb28a-e748-4c51-a7aa-41b5968ef611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating Sample Data\n",
    "data = [\n",
    "    (1, \"Amit\", 50000, \"Sales\"),\n",
    "    (2, \"Neha\", 60000, \"HR\"),\n",
    "    (1, \"Amit\", 50000, \"Sales\"),   # duplicate row\n",
    "    (4, \"Raj\", 70000, \"IT\"),\n",
    "    (5, \"Neha\", 60000, \"HR\"),      # duplicate row\n",
    "    (6, \"Sneha\", 55000, \"Sales\"),\n",
    "    (4, \"Raj\", 70000, \"IT\"),       # duplicate row\n",
    "    (8, \"Amit\", 52000, \"Marketing\"),\n",
    "    (9, \"Neha\", 60000, \"HR\")       # duplicate row\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"salary\", \"dept\"]\n",
    "\n",
    "emp_df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c20fc83-c5f5-42fd-8066-1ae2cef47bad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+---------+\n| id| name|salary|     dept|\n+---+-----+------+---------+\n|  1| Amit| 50000|    Sales|\n|  2| Neha| 60000|       HR|\n|  4|  Raj| 70000|       IT|\n|  5| Neha| 60000|       HR|\n|  6|Sneha| 55000|    Sales|\n|  8| Amit| 52000|Marketing|\n|  9| Neha| 60000|       HR|\n+---+-----+------+---------+\n\n+---+-----+------+---------+\n| id| name|salary|     dept|\n+---+-----+------+---------+\n|  1| Amit| 50000|    Sales|\n|  2| Neha| 60000|       HR|\n|  4|  Raj| 70000|       IT|\n|  5| Neha| 60000|       HR|\n|  6|Sneha| 55000|    Sales|\n|  8| Amit| 52000|Marketing|\n|  9| Neha| 60000|       HR|\n+---+-----+------+---------+\n\n+---+-----+------+-----+\n| id| name|salary| dept|\n+---+-----+------+-----+\n|  1| Amit| 50000|Sales|\n|  2| Neha| 60000|   HR|\n|  4|  Raj| 70000|   IT|\n|  6|Sneha| 55000|Sales|\n+---+-----+------+-----+\n\n+---+-----+------+---------+\n| id| name|salary|     dept|\n+---+-----+------+---------+\n|  1| Amit| 50000|    Sales|\n|  2| Neha| 60000|       HR|\n|  4|  Raj| 70000|       IT|\n|  6|Sneha| 55000|    Sales|\n|  8| Amit| 52000|Marketing|\n+---+-----+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# distinct() - removes duplicate rows from the entire DataFrame\n",
    "# Works on all columns (cannot pass column names)\n",
    "# -------------------------------------------------------------------\n",
    "emp_df.distinct().show()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# dropDuplicates() - also removes duplicate rows from all columns\n",
    "# Same as distinct(), but more commonly used in ETL pipelines\n",
    "# -------------------------------------------------------------------\n",
    "emp_df.dropDuplicates().show()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# dropDuplicates([\"name\"])\n",
    "# Removes duplicate rows based ONLY on the \"name\" column\n",
    "# Keeps the first occurrence of each unique name\n",
    "# -------------------------------------------------------------------\n",
    "emp_df.dropDuplicates(subset=[\"name\"]).show()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# dropDuplicates([\"name\", \"salary\"])\n",
    "# Removes duplicates based on a combination of \"name\" AND \"salary\"\n",
    "# Useful for checking multi-column duplicates\n",
    "# -------------------------------------------------------------------\n",
    "emp_df.dropDuplicates(subset=[\"name\", \"salary\"]).show()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# distinct(\"name\") - NOT allowed\n",
    "# distinct() does NOT accept specific column names\n",
    "# It works only on the entire row, not on a single column\n",
    "# -------------------------------------------------------------------\n",
    "# emp_df.distinct(\"name\").show()  # ❌ This will throw an error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cbb5982-7324-4869-9935-d054c0fb432b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## **_Activity 6 - Removing Null Values_**\n",
    "### - **Using dropna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1430368d-4a3d-4640-87d6-ae21a3bbb930",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+-----+\n|  id| name|salary| dept|\n+----+-----+------+-----+\n|   1| Amit| 50000|Sales|\n|   2| Neha|  NULL|   HR|\n|   3| NULL| 70000|   IT|\n|   4|Sneha| 55000| NULL|\n|   5| NULL|  NULL| NULL|\n|NULL| NULL|  NULL| NULL|\n+----+-----+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Amit\", 50000, \"Sales\"),\n",
    "    (2, \"Neha\", None, \"HR\"),\n",
    "    (3, None, 70000, \"IT\"),\n",
    "    (4, \"Sneha\", 55000, None),\n",
    "    (5, None, None, None),\n",
    "    (None , None, None, None)\n",
    "]\n",
    "columns = [\"id\", \"name\", \"salary\", \"dept\"]\n",
    "emp_df = spark.createDataFrame(data, columns)\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef1fb689-819c-4654-b4e3-1d291fddf520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-----+\n| id|name|salary| dept|\n+---+----+------+-----+\n|  1|Amit| 50000|Sales|\n+---+----+------+-----+\n\n+---+-----+------+-----+\n| id| name|salary| dept|\n+---+-----+------+-----+\n|  1| Amit| 50000|Sales|\n|  2| Neha|  NULL|   HR|\n|  3| NULL| 70000|   IT|\n|  4|Sneha| 55000| NULL|\n|  5| NULL|  NULL| NULL|\n+---+-----+------+-----+\n\n+---+-----+------+-----+\n| id| name|salary| dept|\n+---+-----+------+-----+\n|  1| Amit| 50000|Sales|\n|  3| NULL| 70000|   IT|\n|  4|Sneha| 55000| NULL|\n+---+-----+------+-----+\n\n+---+----+------+-----+\n| id|name|salary| dept|\n+---+----+------+-----+\n|  1|Amit| 50000|Sales|\n|  3|NULL| 70000|   IT|\n+---+----+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# dropna() removes rows that contain NULL values\n",
    "# how = \"any\"  → remove the row if *at least one* column is NULL\n",
    "# This is the default behavior\n",
    "# -------------------------------------------------------------------\n",
    "emp_df.dropna().show()                 # Same as emp_df.dropna(how=\"any\").show()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# how = \"all\"\n",
    "# Remove the row ONLY if *all* columns contain NULL values\n",
    "# If even one column has a non-null value, the row is kept\n",
    "# -------------------------------------------------------------------\n",
    "emp_df.dropna(how=\"all\").show()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# subset = [\"salary\"]\n",
    "# Remove the row if SALARY is NULL\n",
    "# Only checks the specified column(s)\n",
    "# -------------------------------------------------------------------\n",
    "emp_df.dropna(subset=[\"salary\"]).show()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# subset = [\"salary\", \"dept\"]\n",
    "# Remove the row if BOTH salary AND dept have NULL values\n",
    "# If either column is non-null, the row is kept\n",
    "# -------------------------------------------------------------------\n",
    "emp_df.dropna(subset=[\"salary\", \"dept\"]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a95f0ec4-3fc1-4776-aae9-6f4afa386b49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **_Activity 7 - Filter Data_**\n",
    "### - **Using filter**\n",
    "### - **Using Where --> where is alias for filter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfa29c8d-2965-493b-bde2-ce5221f5274e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-----+\n| id| name|salary| dept|\n+---+-----+------+-----+\n|  1| Amit| 50000|Sales|\n|  2| Neha|100000|   HR|\n|  3| ravi| 70000|   IT|\n|  4|Sneha| 55000|   IT|\n|  4|Karan| 60000|Sales|\n+---+-----+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Amit\", 50000, \"Sales\"),\n",
    "    (2, \"Neha\", 100000, \"HR\"),\n",
    "    (3, \"ravi\", 70000, \"IT\"),\n",
    "    (4, \"Sneha\", 55000, \"IT\"),\n",
    "    (4, \"Karan\", 60000, \"Sales\")\n",
    "]\n",
    "columns = [\"id\", \"name\", \"salary\", \"dept\"]\n",
    "emp_df = spark.createDataFrame(data, columns)\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb5fe001-0a3f-4b56-9fac-a6dd0defe386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-----+\n| id| name|salary| dept|\n+---+-----+------+-----+\n|  2| Neha|100000|   HR|\n|  3| ravi| 70000|   IT|\n|  4|Sneha| 55000|   IT|\n|  4|Karan| 60000|Sales|\n+---+-----+------+-----+\n\n+---+-----+------+-----+\n| id| name|salary| dept|\n+---+-----+------+-----+\n|  2| Neha|100000|   HR|\n|  3| ravi| 70000|   IT|\n|  4|Sneha| 55000|   IT|\n|  4|Karan| 60000|Sales|\n+---+-----+------+-----+\n\n+---+----+------+----+\n| id|name|salary|dept|\n+---+----+------+----+\n|  2|Neha|100000|  HR|\n+---+----+------+----+\n\n+---+----+------+----+\n| id|name|salary|dept|\n+---+----+------+----+\n|  2|Neha|100000|  HR|\n+---+----+------+----+\n\n+---+-----+------+-----+\n| id| name|salary| dept|\n+---+-----+------+-----+\n|  1| Amit| 50000|Sales|\n|  2| Neha|100000|   HR|\n|  4|Karan| 60000|Sales|\n+---+-----+------+-----+\n\n+---+-----+------+-----+\n| id| name|salary| dept|\n+---+-----+------+-----+\n|  1| Amit| 50000|Sales|\n|  3| ravi| 70000|   IT|\n|  4|Sneha| 55000|   IT|\n|  4|Karan| 60000|Sales|\n+---+-----+------+-----+\n\n+---+-----+------+-----+\n| id| name|salary| dept|\n+---+-----+------+-----+\n|  1| Amit| 50000|Sales|\n|  4|Sneha| 55000|   IT|\n|  4|Karan| 60000|Sales|\n+---+-----+------+-----+\n\n+---+-----+------+-----+\n| id| name|salary| dept|\n+---+-----+------+-----+\n|  1| Amit| 50000|Sales|\n|  4|Karan| 60000|Sales|\n+---+-----+------+-----+\n\n+---+-----+------+-----+\n| id| name|salary| dept|\n+---+-----+------+-----+\n|  2| Neha|100000|   HR|\n|  3| ravi| 70000|   IT|\n|  4|Sneha| 55000|   IT|\n|  4|Karan| 60000|Sales|\n+---+-----+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Filter rows using SQL expression (string-based condition)\n",
    "# -------------------------------------------------------------------\n",
    "emp_df.filter(\"salary > 50000\").show()\n",
    "emp_df.where(\"salary > 50000\").show()   # where() is an alias of filter()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Filter using column expressions with AND (&)\n",
    "# Each condition must be wrapped in parentheses\n",
    "# -------------------------------------------------------------------\n",
    "emp_df.filter(\n",
    "    (col(\"salary\") > 50000) & (col(\"dept\") == \"HR\")\n",
    ").show()\n",
    "\n",
    "emp_df.where(\n",
    "    (col(\"salary\") > 50000) & (col(\"dept\") == \"HR\")\n",
    ").show()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Filter using OR (|)\n",
    "# Get employees working in HR OR Sales department\n",
    "# -------------------------------------------------------------------\n",
    "emp_df.filter(\n",
    "    (col(\"dept\") == \"HR\") | (col(\"dept\") == \"Sales\")\n",
    ").show()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Using isin() - cleaner alternative to OR\n",
    "# dept column must match any value inside the given list\n",
    "# -------------------------------------------------------------------\n",
    "emp_df.filter(\n",
    "    col(\"dept\").isin([\"Sales\", \"IT\"])\n",
    ").show()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Using between() - inclusive of both lower and upper bounds\n",
    "# Fetch salary between 40,000 and 60,000 (inclusive)\n",
    "# -------------------------------------------------------------------\n",
    "emp_df.filter(\n",
    "    col(\"salary\").between(40000, 60000)\n",
    ").show()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Pattern matching using like()\n",
    "# like(\"S%\")   → starts with 'S'\n",
    "# like(\"%a%\")  → contains 'a'\n",
    "# -------------------------------------------------------------------\n",
    "emp_df.filter(col(\"dept\").like(\"S%\")).show()    # department starts with S\n",
    "emp_df.filter(col(\"name\").like(\"%a%\")).show()   # name contains 'a'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a34e9649-106c-400d-914b-ad07277dd86e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **_Activity 8 - Sorting Rows_**\n",
    "### - **Using sort**\n",
    "### - **Using orderBy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc0a61c4-4015-4ca2-9262-0f2a9a2f0899",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---------+----------+----------+\n| id|  name|salary|  address|department|joineddate|\n+---+------+------+---------+----------+----------+\n|  1|  John| 50000|   Mumbai|        IT|2021-01-15|\n|  2|  Emma| 62000|    Delhi|        HR|2020-03-10|\n|  3|   Raj| 45000|     Pune|   Finance|2022-07-20|\n|  4| Priya| 70000|Bangalore|        IT|2019-11-05|\n|  5|  Alex| 55000|Hyderabad| Marketing|2021-05-12|\n|  6|  Sara| 48000|  Chennai|   Finance|2020-08-25|\n|  7|Nikhil| 72000|Ahmedabad|        HR|2022-02-14|\n|  8| Aisha| 53000|  Kolkata|        IT|2021-09-30|\n|  9|   Tom| 60000|    Surat| Marketing|2023-01-18|\n| 10|  Riya| 58000|   Jaipur|   Finance|2020-12-02|\n| 11|Vikram| 65000|   Nagpur|        HR|2019-04-22|\n| 12|  Zara| 49000|   Indore|        IT|2022-10-11|\n+---+------+------+---------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e883262-ca5d-451e-a527-7010bc6dfb6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---------+----------+----------+\n| id|  name|salary|  address|department|joineddate|\n+---+------+------+---------+----------+----------+\n|  3|   Raj| 45000|     Pune|   Finance|2022-07-20|\n|  6|  Sara| 48000|  Chennai|   Finance|2020-08-25|\n| 12|  Zara| 49000|   Indore|        IT|2022-10-11|\n|  1|  John| 50000|   Mumbai|        IT|2021-01-15|\n|  8| Aisha| 53000|  Kolkata|        IT|2021-09-30|\n|  5|  Alex| 55000|Hyderabad| Marketing|2021-05-12|\n| 10|  Riya| 58000|   Jaipur|   Finance|2020-12-02|\n|  9|   Tom| 60000|    Surat| Marketing|2023-01-18|\n|  2|  Emma| 62000|    Delhi|        HR|2020-03-10|\n| 11|Vikram| 65000|   Nagpur|        HR|2019-04-22|\n|  4| Priya| 70000|Bangalore|        IT|2019-11-05|\n|  7|Nikhil| 72000|Ahmedabad|        HR|2022-02-14|\n+---+------+------+---------+----------+----------+\n\n+---+------+------+---------+----------+----------+\n| id|  name|salary|  address|department|joineddate|\n+---+------+------+---------+----------+----------+\n|  7|Nikhil| 72000|Ahmedabad|        HR|2022-02-14|\n|  4| Priya| 70000|Bangalore|        IT|2019-11-05|\n| 11|Vikram| 65000|   Nagpur|        HR|2019-04-22|\n|  2|  Emma| 62000|    Delhi|        HR|2020-03-10|\n|  9|   Tom| 60000|    Surat| Marketing|2023-01-18|\n| 10|  Riya| 58000|   Jaipur|   Finance|2020-12-02|\n|  5|  Alex| 55000|Hyderabad| Marketing|2021-05-12|\n|  8| Aisha| 53000|  Kolkata|        IT|2021-09-30|\n|  1|  John| 50000|   Mumbai|        IT|2021-01-15|\n| 12|  Zara| 49000|   Indore|        IT|2022-10-11|\n|  6|  Sara| 48000|  Chennai|   Finance|2020-08-25|\n|  3|   Raj| 45000|     Pune|   Finance|2022-07-20|\n+---+------+------+---------+----------+----------+\n\n+---+------+------+---------+----------+----------+\n| id|  name|salary|  address|department|joineddate|\n+---+------+------+---------+----------+----------+\n|  3|   Raj| 45000|     Pune|   Finance|2022-07-20|\n|  6|  Sara| 48000|  Chennai|   Finance|2020-08-25|\n| 10|  Riya| 58000|   Jaipur|   Finance|2020-12-02|\n|  2|  Emma| 62000|    Delhi|        HR|2020-03-10|\n| 11|Vikram| 65000|   Nagpur|        HR|2019-04-22|\n|  7|Nikhil| 72000|Ahmedabad|        HR|2022-02-14|\n| 12|  Zara| 49000|   Indore|        IT|2022-10-11|\n|  1|  John| 50000|   Mumbai|        IT|2021-01-15|\n|  8| Aisha| 53000|  Kolkata|        IT|2021-09-30|\n|  4| Priya| 70000|Bangalore|        IT|2019-11-05|\n|  5|  Alex| 55000|Hyderabad| Marketing|2021-05-12|\n|  9|   Tom| 60000|    Surat| Marketing|2023-01-18|\n+---+------+------+---------+----------+----------+\n\n+---+------+------+---------+----------+----------+\n| id|  name|salary|  address|department|joineddate|\n+---+------+------+---------+----------+----------+\n| 10|  Riya| 58000|   Jaipur|   Finance|2020-12-02|\n|  6|  Sara| 48000|  Chennai|   Finance|2020-08-25|\n|  3|   Raj| 45000|     Pune|   Finance|2022-07-20|\n|  7|Nikhil| 72000|Ahmedabad|        HR|2022-02-14|\n| 11|Vikram| 65000|   Nagpur|        HR|2019-04-22|\n|  2|  Emma| 62000|    Delhi|        HR|2020-03-10|\n|  4| Priya| 70000|Bangalore|        IT|2019-11-05|\n|  8| Aisha| 53000|  Kolkata|        IT|2021-09-30|\n|  1|  John| 50000|   Mumbai|        IT|2021-01-15|\n| 12|  Zara| 49000|   Indore|        IT|2022-10-11|\n|  9|   Tom| 60000|    Surat| Marketing|2023-01-18|\n|  5|  Alex| 55000|Hyderabad| Marketing|2021-05-12|\n+---+------+------+---------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Sorting DataFrame by a single column (ascending by default)\n",
    "# -------------------------------------------------------------------\n",
    "df.sort(\"salary\").show()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Sorting by a column in descending order using desc()\n",
    "# -------------------------------------------------------------------\n",
    "df.sort(col(\"salary\").desc()).show()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Sorting by multiple columns\n",
    "# First by 'department', then by 'salary' (both ascending)\n",
    "# -------------------------------------------------------------------\n",
    "df.sort(\"department\", \"salary\").show()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Sorting by multiple columns with mixed order\n",
    "# 1) department → ascending\n",
    "# 2) salary     → descending\n",
    "# -------------------------------------------------------------------\n",
    "df.sort(\"department\", col(\"salary\").desc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd2b3ce4-9fc6-4e95-b530-6120efb84d4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Class4 - Transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}